{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ca257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and evaluate forward models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ae6c515",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd846cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read tm data and split it\n",
    "resmat = pd.read_excel(\"tm_results_expanded.xlsx\")\n",
    "X = resmat.iloc[:,:4]*(10**9)\n",
    "Y = resmat.iloc[:,11:]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=42);\n",
    "X_train_1, X_test_1, Y_train_1, Y_test_1 = train_test_split(X, Y, random_state=None);\n",
    "X_train_2, X_test_2, Y_train_2, Y_test_2 = train_test_split(X, Y, random_state=None);\n",
    "X_train_3, X_test_3, Y_train_3, Y_test_3 = train_test_split(X, Y, random_state=None);\n",
    "X_train_4, X_test_4, Y_train_4, Y_test_4 = train_test_split(X, Y, random_state=None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecb4d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale train and test data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test_1 = scaler.transform(X_test_1)\n",
    "X_test_2 = scaler.transform(X_test_2)\n",
    "X_test_3 = scaler.transform(X_test_3)\n",
    "X_test_4 = scaler.transform(X_test_4)\n",
    "r = np.amax(X_train, axis = 0) - np.amin(X_train, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80089d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(alpha=0.05, hidden_layer_sizes=(100, 100, 100), max_iter=1000,\n",
       "             solver='lbfgs')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train reference model - 3 hidden layers of 100 neurons each, 1000 iterations\n",
    "nn_ref = MLPRegressor(solver=\"lbfgs\", max_iter=1000, alpha=0.05, hidden_layer_sizes=(100,100,100))\n",
    "nn_ref.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f76b352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9234166266910083\n",
      "[0.9171038198413188, 0.923666538749603, 0.9196428673035602, 0.9288482865762111, 0.9278216209843491]\n"
     ]
    }
   ],
   "source": [
    "#evaluate reference model\n",
    "ref_score = [nn_ref.score(X_test, Y_test), nn_ref.score(X_test_1, Y_test_1), nn_ref.score(X_test_2, Y_test_2),\n",
    "            nn_ref.score(X_test_3, Y_test_3), nn_ref.score(X_test_4, Y_test_4)]\n",
    "print(np.average(ref_score))\n",
    "print(ref_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a089812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a list of neuron layers build\n",
    "def hls_list(min_l=2, max_l=5, step_l=1, min_n=100, max_n=500, step_n=100):\n",
    "    hls = list()\n",
    "    for layer in np.arange(min_l, max_l+1, step_l):\n",
    "        for neuron in np.arange(min_n, max_n+1, step_n):\n",
    "            tup = tuple()\n",
    "            for i in range(layer):\n",
    "                tup = tup + (neuron,)\n",
    "            hls.append(tup)\n",
    "    return hls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a133d67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saarpop/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/saarpop/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/saarpop/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/saarpop/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/saarpop/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/saarpop/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/saarpop/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/saarpop/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/saarpop/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/saarpop/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/saarpop/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try 1:\n",
      "{'mean_fit_time': array([ 0.22133904,  0.23356776,  0.23279657,  0.20459447,  0.24038587,\n",
      "        0.21025872,  0.23469787,  0.22874274,  0.32195392,  0.33506532,\n",
      "        0.30285139,  0.2941442 ,  0.38873997,  0.38968735,  0.3754375 ,\n",
      "        0.38234634,  0.46606936,  0.48500967,  0.49843531,  0.79468136,\n",
      "        1.58221903,  1.93184328,  1.65714383,  2.10563693,  4.22151041,\n",
      "        7.01039491,  9.24255137,  9.03369508, 10.8866044 , 16.4134809 ,\n",
      "       14.8168252 ,  8.88196821]), 'std_fit_time': array([0.02601463, 0.02623877, 0.03405645, 0.04569359, 0.0366831 ,\n",
      "       0.06482824, 0.04585067, 0.04128138, 0.0371722 , 0.02507235,\n",
      "       0.04492881, 0.02305972, 0.05819073, 0.06803271, 0.07314919,\n",
      "       0.03677966, 0.06302396, 0.04513261, 0.06414534, 0.13228924,\n",
      "       0.25069388, 0.18967239, 0.09161909, 0.13878529, 0.6333892 ,\n",
      "       2.48376286, 1.43631093, 1.96257518, 1.5529316 , 4.69925907,\n",
      "       3.88614807, 2.04166678]), 'mean_score_time': array([0.00199599, 0.00102463, 0.00088339, 0.00087862, 0.00090799,\n",
      "       0.00091052, 0.00089741, 0.00101352, 0.0009387 , 0.00091314,\n",
      "       0.00093331, 0.00090742, 0.00095043, 0.0011168 , 0.00097823,\n",
      "       0.00095024, 0.00092287, 0.00115519, 0.00104394, 0.00480533,\n",
      "       0.00162535, 0.00182819, 0.00202165, 0.00181513, 0.00267267,\n",
      "       0.00436306, 0.00538659, 0.0047895 , 0.00555959, 0.00624843,\n",
      "       0.00534859, 0.00407319]), 'std_score_time': array([1.18273858e-03, 1.15179085e-04, 5.75612569e-05, 4.04967555e-05,\n",
      "       3.93451628e-05, 3.58925690e-05, 3.10969491e-05, 2.79061996e-04,\n",
      "       4.42851584e-05, 3.51189839e-05, 1.00265668e-04, 3.74992333e-05,\n",
      "       4.71120421e-05, 2.87295769e-04, 2.47987415e-05, 6.61090654e-05,\n",
      "       2.62478824e-05, 2.06643004e-04, 7.80324159e-05, 6.53348340e-03,\n",
      "       5.32309242e-04, 7.17442000e-04, 3.87010967e-04, 2.73419083e-04,\n",
      "       4.06421964e-04, 2.23345527e-03, 2.30461470e-03, 1.32675952e-03,\n",
      "       2.48352349e-03, 3.61339044e-03, 2.43015337e-03, 3.10238376e-03]), 'param_hidden_layer_sizes': masked_array(data=[(50,), (50,), (50,), (50,), (100,), (100,), (100,),\n",
      "                   (100,), (150,), (150,), (150,), (150,), (200,), (200,),\n",
      "                   (200,), (200,), (50, 50), (50, 50), (50, 50), (50, 50),\n",
      "                   (100, 100), (100, 100), (100, 100), (100, 100),\n",
      "                   (150, 150), (150, 150), (150, 150), (150, 150),\n",
      "                   (200, 200), (200, 200), (200, 200), (200, 200)],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_max_iter': masked_array(data=[500, 1000, 1500, 2000, 500, 1000, 1500, 2000, 500,\n",
      "                   1000, 1500, 2000, 500, 1000, 1500, 2000, 500, 1000,\n",
      "                   1500, 2000, 500, 1000, 1500, 2000, 500, 1000, 1500,\n",
      "                   2000, 500, 1000, 1500, 2000],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'hidden_layer_sizes': (50,), 'max_iter': 500}, {'hidden_layer_sizes': (50,), 'max_iter': 1000}, {'hidden_layer_sizes': (50,), 'max_iter': 1500}, {'hidden_layer_sizes': (50,), 'max_iter': 2000}, {'hidden_layer_sizes': (100,), 'max_iter': 500}, {'hidden_layer_sizes': (100,), 'max_iter': 1000}, {'hidden_layer_sizes': (100,), 'max_iter': 1500}, {'hidden_layer_sizes': (100,), 'max_iter': 2000}, {'hidden_layer_sizes': (150,), 'max_iter': 500}, {'hidden_layer_sizes': (150,), 'max_iter': 1000}, {'hidden_layer_sizes': (150,), 'max_iter': 1500}, {'hidden_layer_sizes': (150,), 'max_iter': 2000}, {'hidden_layer_sizes': (200,), 'max_iter': 500}, {'hidden_layer_sizes': (200,), 'max_iter': 1000}, {'hidden_layer_sizes': (200,), 'max_iter': 1500}, {'hidden_layer_sizes': (200,), 'max_iter': 2000}, {'hidden_layer_sizes': (50, 50), 'max_iter': 500}, {'hidden_layer_sizes': (50, 50), 'max_iter': 1000}, {'hidden_layer_sizes': (50, 50), 'max_iter': 1500}, {'hidden_layer_sizes': (50, 50), 'max_iter': 2000}, {'hidden_layer_sizes': (100, 100), 'max_iter': 500}, {'hidden_layer_sizes': (100, 100), 'max_iter': 1000}, {'hidden_layer_sizes': (100, 100), 'max_iter': 1500}, {'hidden_layer_sizes': (100, 100), 'max_iter': 2000}, {'hidden_layer_sizes': (150, 150), 'max_iter': 500}, {'hidden_layer_sizes': (150, 150), 'max_iter': 1000}, {'hidden_layer_sizes': (150, 150), 'max_iter': 1500}, {'hidden_layer_sizes': (150, 150), 'max_iter': 2000}, {'hidden_layer_sizes': (200, 200), 'max_iter': 500}, {'hidden_layer_sizes': (200, 200), 'max_iter': 1000}, {'hidden_layer_sizes': (200, 200), 'max_iter': 1500}, {'hidden_layer_sizes': (200, 200), 'max_iter': 2000}], 'split0_test_score': array([0.89515973, 0.88058118, 0.89074946, 0.87605162, 0.8981167 ,\n",
      "       0.89233474, 0.8996567 , 0.90624842, 0.90920429, 0.90980467,\n",
      "       0.90983252, 0.90202866, 0.90756597, 0.91184919, 0.9073663 ,\n",
      "       0.91038438, 0.92194897, 0.91673357, 0.91324269, 0.91453193,\n",
      "       0.91706805, 0.92261871, 0.91892226, 0.92657659, 0.92211612,\n",
      "       0.92772393, 0.92972237, 0.92310906, 0.92916951, 0.9197946 ,\n",
      "       0.92487093, 0.89675735]), 'split1_test_score': array([0.90829993, 0.90285523, 0.90910843, 0.92587229, 0.91714042,\n",
      "       0.90818529, 0.90836449, 0.91866507, 0.92413139, 0.92094725,\n",
      "       0.92181749, 0.92237535, 0.92576309, 0.92686816, 0.92236537,\n",
      "       0.92670302, 0.94201424, 0.93355893, 0.9440575 , 0.94523498,\n",
      "       0.9477741 , 0.94307855, 0.94503556, 0.94569194, 0.94016928,\n",
      "       0.94349198, 0.94742145, 0.95556884, 0.93355527, 0.93863473,\n",
      "       0.94567153, 0.94443673]), 'split2_test_score': array([0.91502742, 0.91966622, 0.91232539, 0.92570148, 0.9186812 ,\n",
      "       0.92814326, 0.9264681 , 0.92540444, 0.93119188, 0.9299072 ,\n",
      "       0.93093158, 0.93316231, 0.92753465, 0.93368887, 0.93117789,\n",
      "       0.93633096, 0.94034121, 0.94147629, 0.94364407, 0.94110074,\n",
      "       0.9429832 , 0.94072357, 0.9410432 , 0.94236723, 0.93457123,\n",
      "       0.94209506, 0.94212609, 0.92830874, 0.93216414, 0.94148527,\n",
      "       0.93829846, 0.9415256 ]), 'split3_test_score': array([0.93442171, 0.93987151, 0.93430836, 0.92350398, 0.94437822,\n",
      "       0.94527049, 0.94576865, 0.94027928, 0.95169835, 0.94920963,\n",
      "       0.94707456, 0.94350024, 0.95422091, 0.95633257, 0.94751929,\n",
      "       0.95005238, 0.94403142, 0.95155242, 0.95471654, 0.95315165,\n",
      "       0.95354334, 0.95416184, 0.94683455, 0.95671772, 0.9552833 ,\n",
      "       0.95697673, 0.95274472, 0.95141855, 0.90909027, 0.95668707,\n",
      "       0.90866899, 0.9556738 ]), 'split4_test_score': array([0.91927225, 0.92271323, 0.92480141, 0.91700894, 0.93089787,\n",
      "       0.93115604, 0.92783922, 0.92867686, 0.94026346, 0.93392764,\n",
      "       0.93374108, 0.9394285 , 0.93808448, 0.93550906, 0.93883974,\n",
      "       0.93750045, 0.94408252, 0.94629003, 0.94601707, 0.94207349,\n",
      "       0.94037252, 0.94623859, 0.94350503, 0.9478489 , 0.92598791,\n",
      "       0.91895039, 0.93654928, 0.94516542, 0.94751803, 0.95307949,\n",
      "       0.94232142, 0.94973261]), 'mean_test_score': array([0.91443621, 0.91313747, 0.91425861, 0.91362766, 0.92184288,\n",
      "       0.92101796, 0.92161943, 0.92385481, 0.93129787, 0.92875928,\n",
      "       0.92867944, 0.92809901, 0.93063382, 0.93284957, 0.92945372,\n",
      "       0.93219424, 0.93848367, 0.93792225, 0.94033557, 0.93921856,\n",
      "       0.94034824, 0.94136425, 0.93906812, 0.94384048, 0.93562557,\n",
      "       0.93784762, 0.94171278, 0.94071412, 0.93029944, 0.94193623,\n",
      "       0.93196627, 0.93762522]), 'std_test_score': array([0.01290266, 0.02007319, 0.01480598, 0.01906117, 0.01539399,\n",
      "       0.01859645, 0.0161458 , 0.01124576, 0.01439484, 0.01316474,\n",
      "       0.01242067, 0.01485675, 0.01534482, 0.0143946 , 0.01382414,\n",
      "       0.01319094, 0.00838375, 0.01213615, 0.01412738, 0.01304953,\n",
      "       0.0124753 , 0.01041424, 0.01025057, 0.00985425, 0.0116925 ,\n",
      "       0.01323191, 0.00806096, 0.0127976 , 0.01234231, 0.01298549,\n",
      "       0.01362547, 0.02099587]), 'rank_test_score': array([29, 32, 30, 31, 26, 28, 27, 25, 18, 22, 23, 24, 19, 15, 21, 16, 10,\n",
      "       11,  7,  8,  6,  4,  9,  1, 14, 12,  3,  5, 20,  2, 17, 13],\n",
      "      dtype=int32)}\n",
      "MLPRegressor(alpha=0.05, hidden_layer_sizes=(100, 100), max_iter=2000,\n",
      "             solver='lbfgs')\n",
      "0.9438404750646823\n",
      "{'hidden_layer_sizes': (100, 100), 'max_iter': 2000}\n"
     ]
    }
   ],
   "source": [
    "#performing grid search to avoid overfitting - try 1\n",
    "hls = hls_list(1,2,1,50,200,50)\n",
    "m = [500, 1000, 1500, 2000]\n",
    "\n",
    "par = {'hidden_layer_sizes':hls, 'max_iter':m}\n",
    "\n",
    "nnl = MLPRegressor(solver=\"lbfgs\", alpha=0.05)\n",
    "regl_1 = GridSearchCV(nnl, par, scoring='r2', cv=5)\n",
    "regl_1.fit(X_train,Y_train)\n",
    "print(\"try 1:\")\n",
    "print(regl_1.cv_results_)\n",
    "print(regl_1.best_estimator_)\n",
    "print(regl_1.best_score_)\n",
    "print(regl_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02184676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try 2:\n",
      "{'mean_fit_time': array([ 0.19075189,  0.21492414,  0.25533504,  0.31385059,  0.30694718,\n",
      "        0.32699142,  0.3445509 ,  0.36271157,  0.38317938,  0.41351047,\n",
      "        0.41940584,  0.43773594,  0.51108418,  0.51479802,  0.61761322,\n",
      "        1.82380843,  1.84998083,  2.14473424,  4.73680606,  4.69504719,\n",
      "        4.35755715,  8.6428555 ,  8.89828992,  9.1624011 ,  1.33106461,\n",
      "        1.40099516,  1.39297032,  4.43224883,  5.09260068,  5.88536863,\n",
      "       12.87838459, 13.59945173,  8.07410212, 22.49426022, 22.3710556 ,\n",
      "       28.70034227]), 'std_fit_time': array([ 0.01926635,  0.02977282,  0.04862976,  0.06764048,  0.05992984,\n",
      "        0.0179921 ,  0.03226616,  0.04354156,  0.02194926,  0.0393813 ,\n",
      "        0.04307878,  0.08307712,  0.07987084,  0.08331658,  0.12981845,\n",
      "        0.11222486,  0.18401399,  0.47330171,  0.66713664,  0.9825225 ,\n",
      "        0.4087837 ,  1.3767522 ,  0.60119158,  2.01584654,  0.33962212,\n",
      "        0.25345757,  0.23037378,  0.60214681,  1.04496748,  1.45311608,\n",
      "        2.76967421,  3.82109749,  4.72901884, 11.16955656, 12.5065604 ,\n",
      "       15.32202581]), 'mean_score_time': array([0.00100002, 0.00089264, 0.00098009, 0.00109639, 0.00107384,\n",
      "       0.00129337, 0.00130768, 0.00104442, 0.00107303, 0.00110245,\n",
      "       0.0010725 , 0.00106225, 0.00145321, 0.00120854, 0.00111561,\n",
      "       0.00141354, 0.0014708 , 0.00163455, 0.00203738, 0.00224414,\n",
      "       0.0019166 , 0.00245066, 0.00250406, 0.00243969, 0.00173635,\n",
      "       0.00206966, 0.0019208 , 0.00238223, 0.00241838, 0.004041  ,\n",
      "       0.00352879, 0.00325599, 0.00456605, 0.00468054, 0.00465083,\n",
      "       0.0083509 ]), 'std_score_time': array([2.59347325e-04, 5.04544640e-05, 7.88626386e-05, 6.91000387e-05,\n",
      "       6.14911051e-05, 5.22976235e-04, 5.34697354e-04, 2.45247322e-05,\n",
      "       3.36373383e-05, 1.07227096e-04, 4.43670263e-05, 6.47004243e-05,\n",
      "       5.13482186e-04, 1.33177042e-04, 2.47420137e-05, 1.12801109e-04,\n",
      "       2.20940479e-04, 6.18701742e-04, 3.95391480e-04, 7.03577090e-04,\n",
      "       6.82180317e-04, 5.95383989e-04, 7.50156883e-04, 5.69317596e-04,\n",
      "       1.14741269e-04, 2.80545650e-04, 2.92834609e-04, 2.63427712e-04,\n",
      "       3.93808911e-04, 2.58326479e-03, 1.77656411e-03, 6.10450100e-04,\n",
      "       2.63846061e-03, 2.90085076e-04, 5.56976562e-04, 5.18803280e-03]), 'param_hidden_layer_sizes': masked_array(data=[(50,), (50,), (50,), (100,), (100,), (100,), (150,),\n",
      "                   (150,), (150,), (200,), (200,), (200,), (50, 50),\n",
      "                   (50, 50), (50, 50), (100, 100), (100, 100), (100, 100),\n",
      "                   (150, 150), (150, 150), (150, 150), (200, 200),\n",
      "                   (200, 200), (200, 200), (50, 50, 50), (50, 50, 50),\n",
      "                   (50, 50, 50), (100, 100, 100), (100, 100, 100),\n",
      "                   (100, 100, 100), (150, 150, 150), (150, 150, 150),\n",
      "                   (150, 150, 150), (200, 200, 200), (200, 200, 200),\n",
      "                   (200, 200, 200)],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_max_iter': masked_array(data=[2000, 2500, 3000, 2000, 2500, 3000, 2000, 2500, 3000,\n",
      "                   2000, 2500, 3000, 2000, 2500, 3000, 2000, 2500, 3000,\n",
      "                   2000, 2500, 3000, 2000, 2500, 3000, 2000, 2500, 3000,\n",
      "                   2000, 2500, 3000, 2000, 2500, 3000, 2000, 2500, 3000],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'hidden_layer_sizes': (50,), 'max_iter': 2000}, {'hidden_layer_sizes': (50,), 'max_iter': 2500}, {'hidden_layer_sizes': (50,), 'max_iter': 3000}, {'hidden_layer_sizes': (100,), 'max_iter': 2000}, {'hidden_layer_sizes': (100,), 'max_iter': 2500}, {'hidden_layer_sizes': (100,), 'max_iter': 3000}, {'hidden_layer_sizes': (150,), 'max_iter': 2000}, {'hidden_layer_sizes': (150,), 'max_iter': 2500}, {'hidden_layer_sizes': (150,), 'max_iter': 3000}, {'hidden_layer_sizes': (200,), 'max_iter': 2000}, {'hidden_layer_sizes': (200,), 'max_iter': 2500}, {'hidden_layer_sizes': (200,), 'max_iter': 3000}, {'hidden_layer_sizes': (50, 50), 'max_iter': 2000}, {'hidden_layer_sizes': (50, 50), 'max_iter': 2500}, {'hidden_layer_sizes': (50, 50), 'max_iter': 3000}, {'hidden_layer_sizes': (100, 100), 'max_iter': 2000}, {'hidden_layer_sizes': (100, 100), 'max_iter': 2500}, {'hidden_layer_sizes': (100, 100), 'max_iter': 3000}, {'hidden_layer_sizes': (150, 150), 'max_iter': 2000}, {'hidden_layer_sizes': (150, 150), 'max_iter': 2500}, {'hidden_layer_sizes': (150, 150), 'max_iter': 3000}, {'hidden_layer_sizes': (200, 200), 'max_iter': 2000}, {'hidden_layer_sizes': (200, 200), 'max_iter': 2500}, {'hidden_layer_sizes': (200, 200), 'max_iter': 3000}, {'hidden_layer_sizes': (50, 50, 50), 'max_iter': 2000}, {'hidden_layer_sizes': (50, 50, 50), 'max_iter': 2500}, {'hidden_layer_sizes': (50, 50, 50), 'max_iter': 3000}, {'hidden_layer_sizes': (100, 100, 100), 'max_iter': 2000}, {'hidden_layer_sizes': (100, 100, 100), 'max_iter': 2500}, {'hidden_layer_sizes': (100, 100, 100), 'max_iter': 3000}, {'hidden_layer_sizes': (150, 150, 150), 'max_iter': 2000}, {'hidden_layer_sizes': (150, 150, 150), 'max_iter': 2500}, {'hidden_layer_sizes': (150, 150, 150), 'max_iter': 3000}, {'hidden_layer_sizes': (200, 200, 200), 'max_iter': 2000}, {'hidden_layer_sizes': (200, 200, 200), 'max_iter': 2500}, {'hidden_layer_sizes': (200, 200, 200), 'max_iter': 3000}], 'split0_test_score': array([0.88337917, 0.88496714, 0.89027977, 0.90608731, 0.90254215,\n",
      "       0.90843703, 0.90403648, 0.90146769, 0.90270389, 0.90584607,\n",
      "       0.90818928, 0.91087386, 0.91662541, 0.90863931, 0.90859032,\n",
      "       0.91799531, 0.92476075, 0.92272779, 0.9190501 , 0.93070951,\n",
      "       0.92561734, 0.90936295, 0.92554941, 0.92538456, 0.91956117,\n",
      "       0.91510252, 0.92369953, 0.92005436, 0.91075281, 0.91574036,\n",
      "       0.9168867 , 0.92579558, 0.91054236, 0.91761193, 0.92206603,\n",
      "       0.87864514]), 'split1_test_score': array([0.90696104, 0.89991252, 0.88365503, 0.91054863, 0.91503092,\n",
      "       0.91747131, 0.92340924, 0.92502764, 0.91921046, 0.91984067,\n",
      "       0.92812731, 0.92854   , 0.94027912, 0.93973105, 0.9466723 ,\n",
      "       0.94653347, 0.94332919, 0.94676723, 0.95127618, 0.94533218,\n",
      "       0.9469359 , 0.95053573, 0.95148464, 0.94533226, 0.94900604,\n",
      "       0.94619246, 0.94559166, 0.94804964, 0.9466368 , 0.94503436,\n",
      "       0.94753625, 0.946766  , 0.94948476, 0.90960715, 0.90275489,\n",
      "       0.87092758]), 'split2_test_score': array([0.9084012 , 0.92278198, 0.92044838, 0.93075645, 0.92393189,\n",
      "       0.92781583, 0.9277695 , 0.92536715, 0.92696171, 0.92990235,\n",
      "       0.93000099, 0.92902634, 0.94310011, 0.94255752, 0.93723769,\n",
      "       0.93818531, 0.93702589, 0.943333  , 0.94251296, 0.94223643,\n",
      "       0.93452085, 0.94575997, 0.94339746, 0.94065621, 0.93387428,\n",
      "       0.93781802, 0.93711507, 0.93987191, 0.93750084, 0.94265887,\n",
      "       0.94304538, 0.91436905, 0.90710817, 0.87267288, 0.93200588,\n",
      "       0.9400817 ]), 'split3_test_score': array([0.9202844 , 0.93173808, 0.94046892, 0.94217582, 0.94494214,\n",
      "       0.94191013, 0.94317037, 0.9436764 , 0.95004968, 0.95035316,\n",
      "       0.95016971, 0.94857026, 0.94379259, 0.95803717, 0.95112839,\n",
      "       0.95248188, 0.94705013, 0.95298987, 0.95343396, 0.94376576,\n",
      "       0.95273664, 0.94217525, 0.94790668, 0.95463939, 0.95070933,\n",
      "       0.94837791, 0.94427955, 0.95349895, 0.95305245, 0.9577923 ,\n",
      "       0.95505118, 0.95690967, 0.93854526, 0.92052218, 0.91700638,\n",
      "       0.95253296]), 'split4_test_score': array([0.91980078, 0.90448946, 0.92043654, 0.92686666, 0.9263938 ,\n",
      "       0.93344831, 0.93348687, 0.94004954, 0.92874683, 0.93835101,\n",
      "       0.93840124, 0.93779361, 0.94578212, 0.93309638, 0.94324859,\n",
      "       0.94188114, 0.94731573, 0.95069027, 0.94394771, 0.94578693,\n",
      "       0.94660359, 0.94516474, 0.94988384, 0.94709754, 0.94355096,\n",
      "       0.94790969, 0.95353334, 0.9497548 , 0.94040886, 0.95461048,\n",
      "       0.95254428, 0.94959482, 0.89712233, 0.95049908, 0.94987014,\n",
      "       0.95000909]), 'mean_test_score': array([0.90776532, 0.90877784, 0.91105773, 0.92328697, 0.92256818,\n",
      "       0.92581652, 0.92637449, 0.92711768, 0.92553451, 0.92885865,\n",
      "       0.9309777 , 0.93096081, 0.93791587, 0.93641229, 0.93737546,\n",
      "       0.93941542, 0.93989634, 0.94330163, 0.94204418, 0.94156616,\n",
      "       0.94128286, 0.93859973, 0.94364441, 0.94262199, 0.93934036,\n",
      "       0.93908012, 0.94084383, 0.94224593, 0.93767035, 0.94316727,\n",
      "       0.94301276, 0.93868702, 0.92056058, 0.91418264, 0.92474066,\n",
      "       0.91843929]), 'std_test_score': array([0.01339638, 0.01665666, 0.02108931, 0.01329278, 0.01396382,\n",
      "       0.0117723 , 0.01298008, 0.01487803, 0.01532697, 0.01525851,\n",
      "       0.01379734, 0.01240861, 0.01079035, 0.01611674, 0.01509087,\n",
      "       0.01172871, 0.00843011, 0.01080508, 0.01222725, 0.0055705 ,\n",
      "       0.00982435, 0.0148621 , 0.00944517, 0.00972407, 0.01150219,\n",
      "       0.01258184, 0.01003195, 0.01195657, 0.01448508, 0.01483791,\n",
      "       0.01370261, 0.01596163, 0.01995365, 0.02495928, 0.01571382,\n",
      "       0.03596766]), 'rank_test_score': array([36, 35, 34, 29, 30, 26, 25, 24, 27, 23, 21, 22, 17, 20, 19, 12, 11,\n",
      "        2,  7,  8,  9, 16,  1,  5, 13, 14, 10,  6, 18,  3,  4, 15, 31, 33,\n",
      "       28, 32], dtype=int32)}\n",
      "MLPRegressor(alpha=0.05, hidden_layer_sizes=(200, 200), max_iter=2500,\n",
      "             solver='lbfgs')\n",
      "0.9436444050125108\n",
      "{'hidden_layer_sizes': (200, 200), 'max_iter': 2500}\n"
     ]
    }
   ],
   "source": [
    "#performing grid search to avoid overfitting - try 2\n",
    "hls = hls_list(1,3,1,50,200,50)\n",
    "m = [2000, 2500, 3000]\n",
    "\n",
    "par = {'hidden_layer_sizes':hls, 'max_iter':m}\n",
    "\n",
    "nnl = MLPRegressor(solver=\"lbfgs\", alpha=0.05)\n",
    "regl_2 = GridSearchCV(nnl, par, scoring='r2', cv=5)\n",
    "regl_2.fit(X_train,Y_train)\n",
    "print(\"try 2:\")\n",
    "print(regl_2.cv_results_)\n",
    "print(regl_2.best_estimator_)\n",
    "print(regl_2.best_score_)\n",
    "print(regl_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb29131a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try 3:\n",
      "{'mean_fit_time': array([ 0.49231176,  0.39713178,  0.34978275,  0.55237021,  0.58515449,\n",
      "        0.57107129,  0.60924716,  0.67413664,  0.57692442,  6.84934063,\n",
      "        6.8922173 ,  6.70762777, 13.55081224, 29.80652933, 30.87259512,\n",
      "       57.5959197 , 56.19969816, 65.51237946]), 'std_fit_time': array([ 0.05736118,  0.0610289 ,  0.0708945 ,  0.0909616 ,  0.07310835,\n",
      "        0.10458827,  0.13396647,  0.08067241,  0.21145845,  1.98303831,\n",
      "        1.28510412,  2.14773607,  4.50496255, 12.96575018,  7.95617538,\n",
      "       19.12729821, 19.51957047, 33.67731493]), 'mean_score_time': array([0.00150805, 0.00103083, 0.00098562, 0.00133457, 0.00102725,\n",
      "       0.00135918, 0.00159545, 0.0016326 , 0.00108261, 0.00223246,\n",
      "       0.00181494, 0.00219908, 0.00309629, 0.00646653, 0.00546517,\n",
      "       0.00677705, 0.00907745, 0.00871639]), 'std_score_time': array([6.71277972e-04, 8.90869900e-05, 1.20424467e-04, 2.90454880e-04,\n",
      "       4.28937752e-05, 2.97485989e-04, 6.89528862e-04, 5.82057691e-04,\n",
      "       1.24799626e-04, 6.74049704e-04, 3.99511023e-04, 2.78143149e-04,\n",
      "       2.92941927e-04, 3.93991033e-03, 2.88104024e-03, 2.73418597e-03,\n",
      "       3.86567596e-03, 3.54190882e-03]), 'param_hidden_layer_sizes': masked_array(data=[(200,), (200,), (200,), (300,), (300,), (300,), (400,),\n",
      "                   (400,), (400,), (200, 200), (200, 200), (200, 200),\n",
      "                   (300, 300), (300, 300), (300, 300), (400, 400),\n",
      "                   (400, 400), (400, 400)],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_max_iter': masked_array(data=[2000, 2500, 3000, 2000, 2500, 3000, 2000, 2500, 3000,\n",
      "                   2000, 2500, 3000, 2000, 2500, 3000, 2000, 2500, 3000],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'hidden_layer_sizes': (200,), 'max_iter': 2000}, {'hidden_layer_sizes': (200,), 'max_iter': 2500}, {'hidden_layer_sizes': (200,), 'max_iter': 3000}, {'hidden_layer_sizes': (300,), 'max_iter': 2000}, {'hidden_layer_sizes': (300,), 'max_iter': 2500}, {'hidden_layer_sizes': (300,), 'max_iter': 3000}, {'hidden_layer_sizes': (400,), 'max_iter': 2000}, {'hidden_layer_sizes': (400,), 'max_iter': 2500}, {'hidden_layer_sizes': (400,), 'max_iter': 3000}, {'hidden_layer_sizes': (200, 200), 'max_iter': 2000}, {'hidden_layer_sizes': (200, 200), 'max_iter': 2500}, {'hidden_layer_sizes': (200, 200), 'max_iter': 3000}, {'hidden_layer_sizes': (300, 300), 'max_iter': 2000}, {'hidden_layer_sizes': (300, 300), 'max_iter': 2500}, {'hidden_layer_sizes': (300, 300), 'max_iter': 3000}, {'hidden_layer_sizes': (400, 400), 'max_iter': 2000}, {'hidden_layer_sizes': (400, 400), 'max_iter': 2500}, {'hidden_layer_sizes': (400, 400), 'max_iter': 3000}], 'split0_test_score': array([0.90792647, 0.90332945, 0.90604454, 0.91329244, 0.91398677,\n",
      "       0.91050457, 0.90867741, 0.91060181, 0.90135565, 0.91730722,\n",
      "       0.92738904, 0.93050691, 0.9168042 , 0.92731158, 0.92609735,\n",
      "       0.91214992, 0.92935905, 0.92745423]), 'split1_test_score': array([0.92849787, 0.92479983, 0.92469294, 0.92973466, 0.9247761 ,\n",
      "       0.92968998, 0.9279698 , 0.92247081, 0.92601335, 0.94420739,\n",
      "       0.94254036, 0.94869186, 0.88120405, 0.84153698, 0.94520338,\n",
      "       0.95287901, 0.94664282, 0.88624993]), 'split2_test_score': array([0.93630546, 0.92634521, 0.93394479, 0.93203101, 0.9287676 ,\n",
      "       0.93199693, 0.93259698, 0.93630453, 0.92873264, 0.9427905 ,\n",
      "       0.94372649, 0.91573463, 0.93878789, 0.94358831, 0.94171733,\n",
      "       0.93211021, 0.94278391, 0.94334372]), 'split3_test_score': array([0.95139442, 0.94505068, 0.94992565, 0.95325686, 0.95045847,\n",
      "       0.95205906, 0.95553891, 0.95089876, 0.95506871, 0.95524142,\n",
      "       0.95128216, 0.95435672, 0.89980052, 0.93720376, 0.94534925,\n",
      "       0.93090032, 0.89668864, 0.95788471]), 'split4_test_score': array([0.9368673 , 0.93665395, 0.93169008, 0.93814616, 0.93697261,\n",
      "       0.94292503, 0.93599166, 0.94040719, 0.93986316, 0.95052007,\n",
      "       0.9501273 , 0.94595606, 0.95225534, 0.94338001, 0.93814009,\n",
      "       0.94913793, 0.92193883, 0.94883447]), 'mean_test_score': array([0.9321983 , 0.92723582, 0.9292596 , 0.93329223, 0.93099231,\n",
      "       0.93343511, 0.93215495, 0.93213662, 0.9302067 , 0.94201332,\n",
      "       0.94301307, 0.93904924, 0.9177704 , 0.91860413, 0.93930148,\n",
      "       0.93543548, 0.92748265, 0.93275341]), 'std_test_score': array([0.01421249, 0.01403077, 0.01424596, 0.01293357, 0.01222681,\n",
      "       0.01399153, 0.01503619, 0.0141065 , 0.01768336, 0.01313892,\n",
      "       0.00853033, 0.01408223, 0.0256589 , 0.03898504, 0.00711255,\n",
      "       0.01460052, 0.01779756, 0.02527051]), 'rank_test_score': array([ 9, 16, 14,  7, 12,  6, 10, 11, 13,  2,  1,  4, 18, 17,  3,  5, 15,\n",
      "        8], dtype=int32)}\n",
      "MLPRegressor(alpha=0.05, hidden_layer_sizes=(200, 200), max_iter=2500,\n",
      "             solver='lbfgs')\n",
      "0.9430130695127582\n",
      "{'hidden_layer_sizes': (200, 200), 'max_iter': 2500}\n"
     ]
    }
   ],
   "source": [
    "#performing grid search to avoid overfitting - try 3\n",
    "hls = hls_list(1,2,1,200,400,100)\n",
    "m = [2000, 2500, 3000]\n",
    "\n",
    "par = {'hidden_layer_sizes':hls, 'max_iter':m}\n",
    "\n",
    "nnl = MLPRegressor(solver=\"lbfgs\", alpha=0.05)\n",
    "regl_3 = GridSearchCV(nnl, par, scoring='r2', cv=5)\n",
    "regl_3.fit(X_train,Y_train)\n",
    "print(\"try 3:\")\n",
    "print(regl_3.cv_results_)\n",
    "print(regl_3.best_estimator_)\n",
    "print(regl_3.best_score_)\n",
    "print(regl_3.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19641484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9657626705776499, 0.9708611956013862, 0.9704482170447103, 0.970598094251965, 0.9691581573471839]\n",
      "[0.9675536593070759, 0.9737266078250628, 0.9721281533807484, 0.9734762293299448, 0.971367854610234]\n",
      "[0.965638717164079, 0.969670342430768, 0.968309801503918, 0.9683972011310087, 0.9652074554537547]\n"
     ]
    }
   ],
   "source": [
    "#Check best model\n",
    "nn_gs_1 = regl_1.best_estimator_\n",
    "nn_gs_2 = regl_2.best_estimator_\n",
    "nn_gs_3 = regl_3.best_estimator_\n",
    "gs_1_score = [nn_gs_1.score(X_test, Y_test), nn_gs_1.score(X_test_1, Y_test_1), nn_gs_1.score(X_test_2, Y_test_2),\n",
    "            nn_gs_1.score(X_test_3, Y_test_3), nn_gs_1.score(X_test_4, Y_test_4)]\n",
    "gs_2_score = [nn_gs_2.score(X_test, Y_test), nn_gs_2.score(X_test_1, Y_test_1), nn_gs_2.score(X_test_2, Y_test_2),\n",
    "            nn_gs_2.score(X_test_3, Y_test_3), nn_gs_2.score(X_test_4, Y_test_4)]\n",
    "gs_3_score = [nn_gs_3.score(X_test, Y_test), nn_gs_3.score(X_test_1, Y_test_1), nn_gs_3.score(X_test_2, Y_test_2),\n",
    "            nn_gs_3.score(X_test_3, Y_test_3), nn_gs_3.score(X_test_4, Y_test_4)]\n",
    "print(gs_1_score)\n",
    "print(gs_2_score)\n",
    "print(gs_3_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d06e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "train_scores, test_scores = list(), list()\n",
    "for al in a:\n",
    "    nnl = MLPRegressor(solver=\"lbfgs\", max_iter=2000, hidden_layer_sizes=al, alpha=0.05)\n",
    "    nnl.fit(X_train,Y_train)\n",
    "    # evaluate on the train dataset\n",
    "    train_yhat = nnl.predict(X_train)\n",
    "    train_acc = r2_score(Y_train, train_yhat)\n",
    "    train_scores.append(train_acc)\n",
    "    # evaluate on the test dataset\n",
    "    test_yhat = nnl.predict(X_test)\n",
    "    test_acc = r2_score(Y_test, test_yhat)\n",
    "    test_scores.append(test_acc)\n",
    "    # summarize progress\n",
    "    print('>%d, train: %.3f, test: %.3f' % (al[0], train_acc, test_acc))\n",
    "\n",
    "# plot of train and test scores vs tree depth\n",
    "pyplot.plot(b, train_scores, '-o', label='Train')\n",
    "pyplot.plot(b, test_scores, '-o', label='Test')\n",
    "pyplot.legend()\n",
    "#pyplot.xscale('log')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10eb8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scaler.inverse_transform(X_test), nn.predict(X_test))\n",
    "print(scaler.inverse_transform(X_train), nn.predict(X_train), Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c10785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create gradient descent process and find optimal geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edf27253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector absolute value\n",
    "def vecabs(a):\n",
    "    s = 0\n",
    "    for el in a:\n",
    "        s = s + pow(el,2)\n",
    "    return np.sqrt(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "852db795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' + relu(abs(g[0]-mu_g[0]) - 1/2*rg[0]) + \\n            relu(abs(g[1]-mu_g[1]) - 1/2*rg[1]) + relu(abs(g[2]-mu_g[2]) - 1/2*rg[2]) + \\n            relu(abs(g[3]-mu_g[3]) - 1/2*rg[3]))'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loss calculation\n",
    "def loss(s, est ,g, mu_g, rg):\n",
    "    sp = est.predict([g])[0];\n",
    "    return (pow((sp[0]-s[0]),2) + pow((sp[1]-s[1]), 2))\n",
    "''' + relu(abs(g[0]-mu_g[0]) - 1/2*rg[0]) + \n",
    "            relu(abs(g[1]-mu_g[1]) - 1/2*rg[1]) + relu(abs(g[2]-mu_g[2]) - 1/2*rg[2]) + \n",
    "            relu(abs(g[3]-mu_g[3]) - 1/2*rg[3]))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "280a61ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ReLU\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26a5493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate single iteration of gradient descent adjustment\n",
    "def grad(s, est, g, delta, mu_g, rg):\n",
    "    del0 = np.array([delta[0], 0, 0, 0])\n",
    "    del1 = np.array([0, delta[1], 0, 0])\n",
    "    del2 = np.array([0, 0, delta[2], 0])\n",
    "    del3 = np.array([0, 0, 0, delta[3]])\n",
    "    del_l0 = loss(s, est, g + del0, mu_g, rg) - loss(s, est, g, mu_g, rg)\n",
    "    del_l1 = loss(s, est, g + del1, mu_g, rg) - loss(s, est, g, mu_g, rg)\n",
    "    del_l2 = loss(s, est, g + del2, mu_g, rg) - loss(s, est, g, mu_g, rg)\n",
    "    del_l3 = loss(s, est, g + del3, mu_g, rg) - loss(s, est, g, mu_g, rg)\n",
    "    return [del_l0/delta[0], del_l1/delta[1], del_l2/delta[2], del_l3/delta[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b2a10ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find optimal g given initial g\n",
    "def opt(s, est, g_ini, delta, lam, mu_g, rg, tol, max_iter):\n",
    "    g_hat = g_ini\n",
    "    prev = 0\n",
    "    curr = loss(s, est, g_hat, mu_g, rg)\n",
    "    count = 0\n",
    "    while(abs(curr-prev) > tol and count < max_iter):\n",
    "        prev = curr\n",
    "        g_hat = g_hat - lam*np.array(grad(s, est, g_hat, delta, mu_g, rg))\n",
    "        curr = loss(s, est, g_hat, mu_g, rg)\n",
    "        count += 1\n",
    "    print(count)\n",
    "    return g_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a35b168",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1\n",
      "594\n",
      "975\n",
      "690\n",
      "490\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "151\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "479\n",
      "806\n",
      "987\n",
      "213\n",
      "1000\n",
      "506\n",
      "933\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "378\n",
      "1000\n",
      "583\n",
      "935\n",
      "930\n",
      "1000\n",
      "1000\n",
      "931\n",
      "333\n",
      "653\n",
      "584\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "812\n",
      "989\n",
      "1000\n",
      "1000\n",
      "950\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "249\n",
      "1000\n",
      "48\n",
      "1000\n",
      "424\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "244\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "431\n",
      "574\n",
      "1000\n",
      "1\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1\n",
      "309\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "701\n",
      "1000\n",
      "1000\n",
      "1\n",
      "1000\n",
      "1000\n",
      "231\n",
      "1000\n",
      "54\n",
      "644\n",
      "1000\n",
      "1000\n",
      "169\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "669\n",
      "837\n",
      "1000\n",
      "517\n",
      "694\n",
      "1000\n",
      "509\n",
      "445\n",
      "1000\n",
      "1000\n",
      "726\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "531\n",
      "1000\n",
      "1000\n",
      "460\n",
      "437\n",
      "1000\n",
      "897\n",
      "1000\n",
      "178\n",
      "1000\n",
      "647\n",
      "1000\n",
      "589\n",
      "1\n",
      "1000\n",
      "1000\n",
      "618\n",
      "500\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "752\n",
      "1000\n",
      "656\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1\n",
      "1000\n",
      "1\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "789\n",
      "549\n",
      "659\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "794\n",
      "1000\n",
      "154\n",
      "1000\n",
      "501\n",
      "157\n",
      "1000\n",
      "1\n",
      "1\n",
      "342\n",
      "1000\n",
      "812\n",
      "1000\n",
      "1000\n",
      "248\n",
      "1\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "604\n",
      "1000\n",
      "1\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "995\n",
      "789\n",
      "1000\n",
      "279\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "770\n",
      "1000\n",
      "880\n",
      "1000\n",
      "805\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "389\n",
      "1000\n",
      "1\n",
      "768\n",
      "798\n",
      "130\n",
      "524\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "684\n",
      "1000\n",
      "1000\n",
      "681\n",
      "1000\n",
      "1000\n",
      "286\n",
      "713\n",
      "996\n",
      "1\n",
      "1000\n",
      "1000\n",
      "805\n",
      "132\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "701\n",
      "373\n",
      "1000\n",
      "720\n",
      "1000\n",
      "1000\n",
      "209\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "694\n",
      "503\n",
      "160\n",
      "1\n",
      "1\n",
      "62\n",
      "9\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "84\n",
      "1000\n",
      "1000\n",
      "999\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "692\n",
      "567\n",
      "378\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "976\n",
      "1000\n",
      "547\n",
      "1000\n",
      "541\n",
      "942\n",
      "735\n",
      "1000\n",
      "724\n",
      "1000\n",
      "1000\n",
      "557\n",
      "1000\n",
      "1\n",
      "940\n",
      "1000\n",
      "259\n",
      "1000\n",
      "160\n",
      "944\n",
      "1000\n",
      "1000\n",
      "902\n",
      "1\n",
      "1000\n",
      "460\n",
      "787\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "319\n",
      "431\n",
      "681\n",
      "1\n",
      "1000\n",
      "1000\n",
      "857\n",
      "1000\n",
      "1\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "586\n",
      "305\n",
      "586\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "879\n",
      "1000\n",
      "1000\n",
      "902\n",
      "1000\n",
      "540\n",
      "1\n",
      "1000\n",
      "966\n",
      "309\n",
      "1000\n",
      "1000\n",
      "509\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "742\n",
      "1000\n",
      "74\n",
      "1000\n",
      "511\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "933\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "223\n",
      "1000\n",
      "644\n",
      "315\n",
      "277\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "762\n",
      "1000\n",
      "1000\n",
      "485\n",
      "377\n",
      "1000\n",
      "1\n",
      "412\n",
      "310\n",
      "790\n",
      "1000\n",
      "1000\n",
      "808\n",
      "677\n",
      "733\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "429\n",
      "1000\n",
      "1000\n",
      "428\n",
      "1000\n",
      "215\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "167\n",
      "1000\n",
      "724\n",
      "231\n",
      "200\n",
      "1000\n",
      "1000\n",
      "453\n",
      "1000\n",
      "790\n",
      "68\n",
      "1000\n",
      "1000\n",
      "670\n",
      "1000\n",
      "778\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "574\n",
      "370\n",
      "375\n",
      "1000\n",
      "785\n",
      "501\n",
      "1000\n",
      "301\n",
      "1000\n",
      "1000\n",
      "998\n",
      "296\n",
      "297\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "569\n",
      "992\n",
      "1000\n",
      "1000\n",
      "802\n",
      "321\n",
      "262\n",
      "196\n",
      "1000\n",
      "1000\n",
      "902\n",
      "1000\n",
      "1000\n",
      "503\n",
      "805\n",
      "1000\n",
      "1000\n",
      "645\n",
      "595\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "301\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "775\n",
      "1\n",
      "1000\n",
      "1000\n",
      "946\n",
      "1000\n",
      "1000\n",
      "823\n",
      "1000\n",
      "599\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "958\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1\n",
      "357\n",
      "788\n",
      "714\n",
      "357\n",
      "1000\n",
      "463\n",
      "945\n",
      "1000\n",
      "393\n",
      "1000\n",
      "231\n",
      "1000\n",
      "1000\n",
      "845\n",
      "239\n",
      "1000\n",
      "891\n",
      "1\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "731\n",
      "1000\n",
      "646\n",
      "300\n",
      "1000\n",
      "504\n",
      "1000\n",
      "1\n",
      "1000\n",
      "1000\n",
      "825\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "358\n",
      "814\n",
      "450\n",
      "1000\n",
      "47\n",
      "1000\n",
      "741\n",
      "629\n",
      "1\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "592\n",
      "1\n",
      "1000\n",
      "1\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "951\n",
      "1000\n",
      "975\n",
      "1000\n",
      "182\n",
      "1000\n",
      "591\n",
      "707\n",
      "1000\n",
      "935\n",
      "116\n",
      "768\n",
      "963\n",
      "423\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "512\n",
      "560\n",
      "908\n",
      "1000\n",
      "1000\n",
      "841\n",
      "470\n",
      "509\n",
      "292\n",
      "1\n",
      "276\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "332\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "771\n",
      "444\n",
      "197\n",
      "1000\n",
      "407\n",
      "591\n",
      "627\n",
      "1000\n",
      "471\n",
      "1000\n",
      "1000\n",
      "543\n",
      "1000\n",
      "904\n",
      "1000\n",
      "932\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "813\n",
      "514\n",
      "725\n",
      "642\n",
      "1000\n",
      "889\n",
      "1000\n",
      "157\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "101\n",
      "814\n",
      "1000\n",
      "236\n",
      "751\n",
      "1000\n",
      "413\n",
      "1000\n",
      "236\n",
      "1000\n",
      "827\n",
      "373\n",
      "1000\n",
      "455\n",
      "767\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "618\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "413\n",
      "1000\n",
      "978\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "321\n",
      "556\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "0.0005040117166741581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.01913309, 0.98825536]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Perform GD on ref model\n",
    "nn = nn_ref\n",
    "mu_g = np.mean(X_train, axis = 0)\n",
    "delta = [1/10000, 1/10000, 1/10000, 1/2000]\n",
    "g_min = X_train[0]\n",
    "loss_min = 1\n",
    "for x in X_train:\n",
    "    g = opt([0,1], nn, x, delta, 0.01, mu_g, r, 1e-5, 1e3)\n",
    "    l = loss([0,1], nn, g, mu_g, r)\n",
    "    if (l < loss_min):\n",
    "        g_min = g\n",
    "        loss_min = l\n",
    "print(loss_min)\n",
    "nn.predict([g_min])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5ca70bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal geometry, according to AI magic: [ 70.00011854 100.00003831  19.99951484 700.00215542]\n",
      "The spectrum predicted by the model: [[0.01913309 0.98825536]]\n"
     ]
    }
   ],
   "source": [
    "#Present optimal reference geometry\n",
    "g_min_ref = g_min\n",
    "print(\"The optimal geometry, according to AI magic:\", scaler.inverse_transform(g_min_ref))\n",
    "print(\"The spectrum predicted by the model:\", nn.predict([g_min_ref]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "214a047c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal simulated geometry [x1, x2, gap, he]: [80, 90, 20, 700]\n",
      "Simulated [p0, p1]: [0.002775338, 0.996192651]\n",
      "Predicted [p0, p1]: [0.02121002 0.97754033]\n",
      "Simulated loss: 2.2198407422045116e-05\n",
      "Predicted loss: 0.0009543014932363147\n",
      "Is predicted better than simulated?: NO!!\n"
     ]
    }
   ],
   "source": [
    "#Compare with best measurement in tm train dataset - real loss = 2.22e-5\n",
    "print(\"Optimal simulated geometry [x1, x2, gap, he]:\", [80,90,20,700])\n",
    "g0 = scaler.transform([[80,90,20,700]])\n",
    "#print(g0[0])\n",
    "sp = nn.predict(g0)[0]\n",
    "ssim = [0.002775338, 0.996192651]\n",
    "print(\"Simulated [p0, p1]:\", ssim)\n",
    "print(\"Predicted [p0, p1]:\", sp)\n",
    "ls = pow((ssim[0]-0),2) + pow((ssim[1]-1), 2)\n",
    "lp = pow((sp[0]-0),2) + pow((sp[1]-1), 2)\n",
    "print(\"Simulated loss:\", ls)\n",
    "print(\"Predicted loss:\", lp)\n",
    "if(ls>=lp):\n",
    "    print(\"Is predicted better than simulated?: YES!\")\n",
    "else:\n",
    "    print(\"Is predicted better than simulated?: NO!!\")\n",
    "\n",
    "#loss([0,1], nn, g0[0], mu_g, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "477211a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "221\n",
      "1000\n",
      "674\n",
      "465\n",
      "1000\n",
      "634\n",
      "826\n",
      "367\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "199\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "368\n",
      "149\n",
      "1000\n",
      "329\n",
      "1000\n",
      "420\n",
      "886\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "795\n",
      "1000\n",
      "729\n",
      "169\n",
      "951\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "253\n",
      "1000\n",
      "567\n",
      "1000\n",
      "195\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "947\n",
      "149\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "138\n",
      "1000\n",
      "1000\n",
      "823\n",
      "1000\n",
      "343\n",
      "1000\n",
      "644\n",
      "1000\n",
      "1000\n",
      "901\n",
      "1000\n",
      "928\n",
      "1000\n",
      "565\n",
      "1000\n",
      "1000\n",
      "814\n",
      "1000\n",
      "567\n",
      "756\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "225\n",
      "1000\n",
      "341\n",
      "130\n",
      "1000\n",
      "260\n",
      "1000\n",
      "698\n",
      "303\n",
      "1000\n",
      "895\n",
      "1000\n",
      "1000\n",
      "1\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "125\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "403\n",
      "1000\n",
      "1000\n",
      "1\n",
      "567\n",
      "1000\n",
      "1000\n",
      "683\n",
      "1000\n",
      "903\n",
      "536\n",
      "272\n",
      "1000\n",
      "1000\n",
      "734\n",
      "1\n",
      "987\n",
      "496\n",
      "1000\n",
      "233\n",
      "977\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "224\n",
      "1000\n",
      "1000\n",
      "224\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "342\n",
      "893\n",
      "633\n",
      "140\n",
      "486\n",
      "205\n",
      "1\n",
      "1000\n",
      "62\n",
      "806\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1\n",
      "963\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "385\n",
      "771\n",
      "1\n",
      "1000\n",
      "1\n",
      "876\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "98\n",
      "611\n",
      "1000\n",
      "434\n",
      "1000\n",
      "723\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "512\n",
      "1000\n",
      "1000\n",
      "134\n",
      "996\n",
      "1\n",
      "1000\n",
      "1000\n",
      "499\n",
      "192\n",
      "1000\n",
      "1\n",
      "91\n",
      "1000\n",
      "528\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1\n",
      "1000\n",
      "1000\n",
      "258\n",
      "1000\n",
      "896\n",
      "1000\n",
      "991\n",
      "1000\n",
      "458\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "795\n",
      "207\n",
      "1000\n",
      "726\n",
      "284\n",
      "1000\n",
      "717\n",
      "1000\n",
      "550\n",
      "1000\n",
      "1\n",
      "806\n",
      "426\n",
      "271\n",
      "1000\n",
      "940\n",
      "1000\n",
      "1000\n",
      "216\n",
      "1000\n",
      "255\n",
      "1\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "103\n",
      "406\n",
      "1000\n",
      "1000\n",
      "638\n",
      "1000\n",
      "1000\n",
      "388\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "518\n",
      "1000\n",
      "1000\n",
      "939\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "213\n",
      "386\n",
      "444\n",
      "1000\n",
      "373\n",
      "1000\n",
      "1000\n",
      "367\n",
      "1000\n",
      "1000\n",
      "942\n",
      "1000\n",
      "871\n",
      "749\n",
      "368\n",
      "1\n",
      "71\n",
      "44\n",
      "1\n",
      "1000\n",
      "32\n",
      "1000\n",
      "1000\n",
      "219\n",
      "623\n",
      "1000\n",
      "294\n",
      "1000\n",
      "357\n",
      "594\n",
      "520\n",
      "640\n",
      "435\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "794\n",
      "1000\n",
      "994\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1\n",
      "1\n",
      "925\n",
      "1000\n",
      "571\n",
      "1000\n",
      "532\n",
      "1000\n",
      "368\n",
      "1000\n",
      "105\n",
      "1000\n",
      "1000\n",
      "295\n",
      "1000\n",
      "888\n",
      "1000\n",
      "1000\n",
      "400\n",
      "1000\n",
      "193\n",
      "1000\n",
      "1000\n",
      "125\n",
      "936\n",
      "1\n",
      "1000\n",
      "243\n",
      "888\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "821\n",
      "1000\n",
      "923\n",
      "591\n",
      "675\n",
      "1\n",
      "1000\n",
      "922\n",
      "1000\n",
      "1000\n",
      "1\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1\n",
      "1000\n",
      "1000\n",
      "535\n",
      "1000\n",
      "759\n",
      "145\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "493\n",
      "1\n",
      "1000\n",
      "1000\n",
      "471\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "482\n",
      "1000\n",
      "760\n",
      "1000\n",
      "64\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "459\n",
      "1000\n",
      "772\n",
      "521\n",
      "310\n",
      "1000\n",
      "1000\n",
      "619\n",
      "327\n",
      "901\n",
      "1\n",
      "1\n",
      "1000\n",
      "1000\n",
      "210\n",
      "1000\n",
      "1000\n",
      "515\n",
      "1\n",
      "1000\n",
      "1\n",
      "881\n",
      "633\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "519\n",
      "936\n",
      "172\n",
      "578\n",
      "1000\n",
      "1000\n",
      "319\n",
      "1000\n",
      "1000\n",
      "423\n",
      "359\n",
      "411\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "339\n",
      "15\n",
      "775\n",
      "297\n",
      "304\n",
      "1000\n",
      "1000\n",
      "923\n",
      "1000\n",
      "304\n",
      "363\n",
      "1000\n",
      "1000\n",
      "325\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "623\n",
      "1000\n",
      "324\n",
      "1000\n",
      "1000\n",
      "350\n",
      "1\n",
      "533\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "141\n",
      "1000\n",
      "545\n",
      "1000\n",
      "451\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "547\n",
      "366\n",
      "589\n",
      "168\n",
      "627\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "955\n",
      "618\n",
      "923\n",
      "301\n",
      "1000\n",
      "351\n",
      "794\n",
      "1000\n",
      "1\n",
      "883\n",
      "1000\n",
      "235\n",
      "408\n",
      "1000\n",
      "1000\n",
      "329\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "889\n",
      "636\n",
      "269\n",
      "1000\n",
      "1000\n",
      "655\n",
      "940\n",
      "1000\n",
      "1000\n",
      "197\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "897\n",
      "895\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "181\n",
      "552\n",
      "1000\n",
      "724\n",
      "1\n",
      "59\n",
      "679\n",
      "534\n",
      "293\n",
      "857\n",
      "535\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "977\n",
      "508\n",
      "648\n",
      "226\n",
      "216\n",
      "1000\n",
      "565\n",
      "1\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "703\n",
      "1000\n",
      "131\n",
      "1000\n",
      "828\n",
      "258\n",
      "1000\n",
      "131\n",
      "1000\n",
      "1\n",
      "1000\n",
      "788\n",
      "752\n",
      "1000\n",
      "214\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "119\n",
      "763\n",
      "1000\n",
      "1000\n",
      "44\n",
      "1000\n",
      "1000\n",
      "659\n",
      "1\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "693\n",
      "238\n",
      "1000\n",
      "1\n",
      "477\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1\n",
      "235\n",
      "1000\n",
      "547\n",
      "540\n",
      "1000\n",
      "1000\n",
      "56\n",
      "361\n",
      "1000\n",
      "194\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "480\n",
      "869\n",
      "1\n",
      "143\n",
      "780\n",
      "698\n",
      "646\n",
      "1000\n",
      "1000\n",
      "277\n",
      "1000\n",
      "187\n",
      "1000\n",
      "999\n",
      "158\n",
      "1000\n",
      "797\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "575\n",
      "425\n",
      "93\n",
      "269\n",
      "827\n",
      "56\n",
      "957\n",
      "900\n",
      "686\n",
      "256\n",
      "1000\n",
      "554\n",
      "464\n",
      "500\n",
      "1000\n",
      "269\n",
      "563\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "438\n",
      "435\n",
      "484\n",
      "1000\n",
      "1000\n",
      "956\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "72\n",
      "168\n",
      "1000\n",
      "1000\n",
      "58\n",
      "1000\n",
      "1000\n",
      "660\n",
      "1000\n",
      "1000\n",
      "731\n",
      "1000\n",
      "293\n",
      "1000\n",
      "795\n",
      "551\n",
      "1000\n",
      "477\n",
      "537\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "745\n",
      "647\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "606\n",
      "553\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1\n",
      "439\n",
      "469\n",
      "385\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "0.0009838007184984688\n",
      "The optimal CV geometry, according to AI magic: [ 72.15390766  82.99058432  21.2108769  718.9143833 ]\n",
      "The spectrum predicted by the model: [[0.03930048 0.94827491]]\n"
     ]
    }
   ],
   "source": [
    "#Perform gradient descent for the smaller model\n",
    "g_min = X_train[0]\n",
    "loss_min = 1\n",
    "for x in X_train:\n",
    "    g = opt([0,1], nn_gs_2, x, delta, 0.01, mu_g, r, 1e-5, 1e3)\n",
    "    l = loss([0,1], nn_gs_2, g, mu_g, r)\n",
    "    if (l < loss_min):\n",
    "        g_min = g\n",
    "        loss_min = l\n",
    "print(loss_min)\n",
    "nn.predict([g_min])\n",
    "#Present optimal CV geometry\n",
    "g_min_ref = g_min\n",
    "print(\"The optimal CV geometry, according to AI magic:\", scaler.inverse_transform(g_min))\n",
    "print(\"The spectrum predicted by the model:\", nn.predict([g_min]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "222b5fc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1092\n",
      "2106\n",
      "1382\n",
      "2397\n",
      "2101\n",
      "1\n",
      "594\n",
      "975\n",
      "690\n",
      "490\n",
      "2798\n",
      "1177\n",
      "1111\n",
      "151\n",
      "2159\n",
      "2621\n",
      "3915\n",
      "1106\n",
      "479\n",
      "806\n",
      "987\n",
      "213\n",
      "1702\n",
      "506\n",
      "933\n",
      "1487\n",
      "2653\n",
      "2064\n",
      "378\n",
      "1761\n",
      "583\n",
      "935\n",
      "930\n",
      "2871\n",
      "1397\n",
      "931\n",
      "333\n",
      "653\n",
      "584\n",
      "1182\n",
      "1180\n",
      "2363\n",
      "1185\n",
      "812\n",
      "989\n",
      "2248\n",
      "2304\n",
      "950\n",
      "1953\n",
      "1968\n",
      "2974\n",
      "1861\n",
      "249\n",
      "1142\n",
      "48\n",
      "2104\n",
      "424\n",
      "2270\n",
      "1109\n",
      "1856\n",
      "1233\n",
      "2498\n",
      "3241\n",
      "244\n",
      "4173\n",
      "1921\n",
      "1521\n",
      "1530\n",
      "431\n",
      "574\n",
      "3590\n",
      "1\n",
      "2678\n",
      "4342\n",
      "3389\n",
      "1927\n",
      "1805\n",
      "2533\n",
      "1268\n",
      "1\n",
      "309\n",
      "1645\n",
      "2012\n",
      "1188\n",
      "701\n",
      "2936\n",
      "2021\n",
      "1\n",
      "2681\n",
      "4434\n",
      "231\n",
      "1107\n",
      "54\n",
      "644\n",
      "1277\n",
      "1551\n",
      "169\n",
      "1503\n",
      "2493\n",
      "2209\n",
      "669\n",
      "837\n",
      "1673\n",
      "517\n",
      "694\n",
      "1664\n",
      "509\n",
      "445\n",
      "1677\n",
      "2654\n",
      "726\n",
      "3631\n",
      "1669\n",
      "2097\n",
      "2048\n",
      "4974\n",
      "1259\n",
      "1742\n",
      "1504\n",
      "1891\n",
      "1795\n",
      "2425\n",
      "531\n",
      "3200\n",
      "1463\n",
      "460\n",
      "437\n",
      "1984\n",
      "897\n",
      "2292\n",
      "178\n",
      "2040\n",
      "647\n",
      "1254\n",
      "589\n",
      "1\n",
      "5830\n",
      "2409\n",
      "618\n",
      "500\n",
      "2396\n",
      "3089\n",
      "1158\n",
      "752\n",
      "2520\n",
      "656\n",
      "2436\n",
      "1371\n",
      "1438\n",
      "1333\n",
      "1283\n",
      "1520\n",
      "1\n",
      "2074\n",
      "1\n",
      "2084\n",
      "1196\n",
      "3001\n",
      "789\n",
      "549\n",
      "659\n",
      "1316\n",
      "1377\n",
      "4904\n",
      "1106\n",
      "3696\n",
      "1416\n",
      "2490\n",
      "794\n",
      "2266\n",
      "154\n",
      "1285\n",
      "501\n",
      "157\n",
      "1149\n",
      "1\n",
      "1\n",
      "342\n",
      "1883\n",
      "812\n",
      "2219\n",
      "1741\n",
      "248\n",
      "1\n",
      "2194\n",
      "2264\n",
      "1914\n",
      "604\n",
      "1511\n",
      "1\n",
      "2439\n",
      "2756\n",
      "3009\n",
      "1934\n",
      "2220\n",
      "995\n",
      "789\n",
      "1608\n",
      "279\n",
      "3327\n",
      "1348\n",
      "2603\n",
      "770\n",
      "1235\n",
      "880\n",
      "1229\n",
      "805\n",
      "4028\n",
      "1831\n",
      "1829\n",
      "2368\n",
      "389\n",
      "2291\n",
      "1\n",
      "768\n",
      "798\n",
      "130\n",
      "524\n",
      "2457\n",
      "1084\n",
      "1804\n",
      "2074\n",
      "684\n",
      "1168\n",
      "2027\n",
      "681\n",
      "3165\n",
      "2486\n",
      "286\n",
      "713\n",
      "996\n",
      "1\n",
      "1229\n",
      "4076\n",
      "805\n",
      "132\n",
      "1709\n",
      "1890\n",
      "1293\n",
      "1545\n",
      "3032\n",
      "1337\n",
      "1012\n",
      "1196\n",
      "2352\n",
      "1314\n",
      "3993\n",
      "701\n",
      "373\n",
      "1910\n",
      "720\n",
      "1237\n",
      "1835\n",
      "209\n",
      "1925\n",
      "1860\n",
      "2998\n",
      "1027\n",
      "694\n",
      "503\n",
      "160\n",
      "1\n",
      "1\n",
      "62\n",
      "9\n",
      "4374\n",
      "2309\n",
      "1833\n",
      "1911\n",
      "84\n",
      "2727\n",
      "1411\n",
      "999\n",
      "1912\n",
      "2826\n",
      "1086\n",
      "692\n",
      "567\n",
      "378\n",
      "1416\n",
      "3283\n",
      "3626\n",
      "1422\n",
      "1118\n",
      "1469\n",
      "1246\n",
      "1120\n",
      "2358\n",
      "6937\n",
      "2285\n",
      "2460\n",
      "2449\n",
      "976\n",
      "1093\n",
      "547\n",
      "1651\n",
      "541\n",
      "942\n",
      "735\n",
      "1769\n",
      "724\n",
      "1478\n",
      "1538\n",
      "557\n",
      "1778\n",
      "1\n",
      "940\n",
      "1381\n",
      "259\n",
      "1745\n",
      "160\n",
      "944\n",
      "2327\n",
      "2474\n",
      "902\n",
      "1\n",
      "1401\n",
      "460\n",
      "787\n",
      "3932\n",
      "1237\n",
      "2330\n",
      "2683\n",
      "2645\n",
      "1782\n",
      "319\n",
      "431\n",
      "681\n",
      "1\n",
      "3314\n",
      "1767\n",
      "857\n",
      "2474\n",
      "1\n",
      "1803\n",
      "2780\n",
      "1099\n",
      "1898\n",
      "1\n",
      "2051\n",
      "1656\n",
      "1303\n",
      "1192\n",
      "586\n",
      "305\n",
      "586\n",
      "1608\n",
      "2248\n",
      "2864\n",
      "879\n",
      "1170\n",
      "1247\n",
      "902\n",
      "1836\n",
      "540\n",
      "1\n",
      "3021\n",
      "966\n",
      "309\n",
      "4173\n",
      "1496\n",
      "509\n",
      "2043\n",
      "2585\n",
      "1500\n",
      "742\n",
      "1983\n",
      "74\n",
      "1935\n",
      "511\n",
      "2200\n",
      "1750\n",
      "3850\n",
      "933\n",
      "1771\n",
      "1155\n",
      "4610\n",
      "223\n",
      "2755\n",
      "644\n",
      "315\n",
      "277\n",
      "2095\n",
      "3820\n",
      "2032\n",
      "3736\n",
      "1203\n",
      "762\n",
      "2083\n",
      "3155\n",
      "485\n",
      "377\n",
      "2402\n",
      "1\n",
      "412\n",
      "310\n",
      "790\n",
      "1492\n",
      "1100\n",
      "808\n",
      "677\n",
      "733\n",
      "2220\n",
      "1587\n",
      "2168\n",
      "429\n",
      "3669\n",
      "1115\n",
      "428\n",
      "1062\n",
      "215\n",
      "1964\n",
      "1423\n",
      "1046\n",
      "167\n",
      "1587\n",
      "724\n",
      "231\n",
      "200\n",
      "1611\n",
      "1880\n",
      "453\n",
      "4044\n",
      "790\n",
      "68\n",
      "2793\n",
      "2737\n",
      "670\n",
      "1537\n",
      "778\n",
      "1886\n",
      "1367\n",
      "3396\n",
      "2378\n",
      "574\n",
      "370\n",
      "375\n",
      "1470\n",
      "785\n",
      "501\n",
      "3405\n",
      "301\n",
      "1758\n",
      "1615\n",
      "998\n",
      "296\n",
      "297\n",
      "1489\n",
      "1524\n",
      "1042\n",
      "569\n",
      "992\n",
      "2427\n",
      "1548\n",
      "802\n",
      "321\n",
      "262\n",
      "196\n",
      "5531\n",
      "1923\n",
      "902\n",
      "2404\n",
      "1200\n",
      "503\n",
      "805\n",
      "1624\n",
      "1377\n",
      "645\n",
      "595\n",
      "1291\n",
      "1745\n",
      "1026\n",
      "1756\n",
      "3271\n",
      "301\n",
      "1473\n",
      "2824\n",
      "2615\n",
      "1395\n",
      "775\n",
      "1\n",
      "3486\n",
      "1023\n",
      "946\n",
      "1713\n",
      "1019\n",
      "823\n",
      "1063\n",
      "599\n",
      "2155\n",
      "2668\n",
      "1963\n",
      "2483\n",
      "1638\n",
      "958\n",
      "2033\n",
      "1742\n",
      "1517\n",
      "1337\n",
      "1586\n",
      "3497\n",
      "2965\n",
      "4403\n",
      "1276\n",
      "1411\n",
      "1\n",
      "357\n",
      "788\n",
      "714\n",
      "357\n",
      "1354\n",
      "463\n",
      "945\n",
      "4068\n",
      "393\n",
      "2047\n",
      "231\n",
      "1247\n",
      "2460\n",
      "845\n",
      "239\n",
      "1001\n",
      "891\n",
      "1\n",
      "1260\n",
      "2317\n",
      "1910\n",
      "1177\n",
      "1919\n",
      "731\n",
      "1730\n",
      "646\n",
      "300\n",
      "3125\n",
      "504\n",
      "2613\n",
      "1\n",
      "1995\n",
      "3684\n",
      "825\n",
      "2806\n",
      "1049\n",
      "1520\n",
      "1516\n",
      "2096\n",
      "1733\n",
      "358\n",
      "814\n",
      "450\n",
      "2961\n",
      "47\n",
      "1193\n",
      "741\n",
      "629\n",
      "1\n",
      "2225\n",
      "2211\n",
      "2434\n",
      "592\n",
      "1\n",
      "2741\n",
      "1\n",
      "2109\n",
      "1134\n",
      "1008\n",
      "951\n",
      "1936\n",
      "975\n",
      "2362\n",
      "182\n",
      "1957\n",
      "591\n",
      "707\n",
      "1618\n",
      "935\n",
      "116\n",
      "768\n",
      "963\n",
      "423\n",
      "1616\n",
      "1252\n",
      "3928\n",
      "3246\n",
      "512\n",
      "560\n",
      "908\n",
      "2300\n",
      "1456\n",
      "841\n",
      "470\n",
      "509\n",
      "292\n",
      "1\n",
      "276\n",
      "3499\n",
      "2236\n",
      "2423\n",
      "2235\n",
      "1389\n",
      "2573\n",
      "332\n",
      "1028\n",
      "2188\n",
      "1071\n",
      "771\n",
      "444\n",
      "197\n",
      "1898\n",
      "407\n",
      "591\n",
      "627\n",
      "1503\n",
      "471\n",
      "2279\n",
      "2538\n",
      "543\n",
      "1188\n",
      "904\n",
      "1599\n",
      "932\n",
      "1698\n",
      "2832\n",
      "1139\n",
      "813\n",
      "514\n",
      "725\n",
      "642\n",
      "2182\n",
      "889\n",
      "1052\n",
      "157\n",
      "1215\n",
      "3573\n",
      "1084\n",
      "1689\n",
      "1775\n",
      "4588\n",
      "101\n",
      "814\n",
      "1429\n",
      "236\n",
      "751\n",
      "4009\n",
      "413\n",
      "3500\n",
      "236\n",
      "1901\n",
      "827\n",
      "373\n",
      "1628\n",
      "455\n",
      "767\n",
      "1083\n",
      "2028\n",
      "1501\n",
      "1711\n",
      "618\n",
      "1176\n",
      "2104\n",
      "1049\n",
      "413\n",
      "1831\n",
      "978\n",
      "1674\n",
      "2337\n",
      "2527\n",
      "1593\n",
      "2889\n",
      "321\n",
      "556\n",
      "2799\n",
      "1519\n",
      "2338\n",
      "0.0005040117166741581\n",
      "The optimal reference geometry, according to AI magic, 10000 tries: [ 70.00011854 100.00003831  19.99951484 700.00215542]\n",
      "The spectrum predicted by the model: [[0.01913309 0.98825536]]\n"
     ]
    }
   ],
   "source": [
    "#Check gradient descent for 10000 tries - better?\n",
    "nn = nn_ref\n",
    "mu_g = np.mean(X_train, axis = 0)\n",
    "delta = [1/10000, 1/10000, 1/10000, 1/2000]\n",
    "g_min = X_train[0]\n",
    "loss_min = 1\n",
    "for x in X_train:\n",
    "    g = opt([0,1], nn, x, delta, 0.01, mu_g, r, 1e-5, 1e4)\n",
    "    l = loss([0,1], nn, g, mu_g, r)\n",
    "    if (l < loss_min):\n",
    "        g_min = g\n",
    "        loss_min = l\n",
    "print(loss_min)\n",
    "nn.predict([g_min])\n",
    "print(\"The optimal reference geometry, according to AI magic, 10000 tries:\", scaler.inverse_transform(g_min))\n",
    "print(\"The spectrum predicted by the model:\", nn.predict([g_min]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c441c3f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n",
      "221\n",
      "1139\n",
      "674\n",
      "465\n",
      "2046\n",
      "634\n",
      "826\n",
      "367\n",
      "1860\n",
      "2657\n",
      "1304\n",
      "1014\n",
      "199\n",
      "2437\n",
      "3766\n",
      "4184\n",
      "1112\n",
      "368\n",
      "149\n",
      "1151\n",
      "329\n",
      "1867\n",
      "420\n",
      "886\n",
      "1906\n",
      "2557\n",
      "4689\n",
      "795\n",
      "1089\n",
      "729\n",
      "169\n",
      "951\n",
      "2724\n",
      "1019\n",
      "1044\n",
      "253\n",
      "1380\n",
      "567\n",
      "1340\n",
      "195\n",
      "1515\n",
      "1285\n",
      "1038\n",
      "947\n",
      "149\n",
      "1703\n",
      "1789\n",
      "2149\n",
      "138\n",
      "2831\n",
      "1371\n",
      "823\n",
      "2174\n",
      "343\n",
      "1178\n",
      "644\n",
      "1700\n",
      "1121\n",
      "901\n",
      "1139\n",
      "928\n",
      "1171\n",
      "565\n",
      "2583\n",
      "2560\n",
      "814\n",
      "1328\n",
      "567\n",
      "756\n",
      "1371\n",
      "1674\n",
      "1125\n",
      "2783\n",
      "225\n",
      "2096\n",
      "341\n",
      "130\n",
      "1970\n",
      "260\n",
      "1536\n",
      "698\n",
      "303\n",
      "1211\n",
      "895\n",
      "1268\n",
      "2392\n",
      "1\n",
      "3135\n",
      "2409\n",
      "1047\n",
      "1766\n",
      "125\n",
      "3826\n",
      "1366\n",
      "1213\n",
      "403\n",
      "1271\n",
      "1693\n",
      "1\n",
      "567\n",
      "1143\n",
      "1240\n",
      "683\n",
      "1493\n",
      "903\n",
      "536\n",
      "272\n",
      "1480\n",
      "1012\n",
      "734\n",
      "1\n",
      "987\n",
      "496\n",
      "1812\n",
      "233\n",
      "977\n",
      "3587\n",
      "2550\n",
      "2091\n",
      "3716\n",
      "3916\n",
      "224\n",
      "3127\n",
      "1347\n",
      "224\n",
      "1741\n",
      "1718\n",
      "1082\n",
      "1182\n",
      "342\n",
      "893\n",
      "633\n",
      "140\n",
      "486\n",
      "205\n",
      "1\n",
      "2910\n",
      "62\n",
      "806\n",
      "2190\n",
      "1070\n",
      "1487\n",
      "2283\n",
      "1\n",
      "963\n",
      "1918\n",
      "1257\n",
      "1521\n",
      "1222\n",
      "385\n",
      "771\n",
      "1\n",
      "1272\n",
      "1\n",
      "876\n",
      "1105\n",
      "1836\n",
      "2054\n",
      "98\n",
      "611\n",
      "2828\n",
      "434\n",
      "2123\n",
      "723\n",
      "1843\n",
      "1594\n",
      "1094\n",
      "1200\n",
      "1058\n",
      "512\n",
      "1275\n",
      "1916\n",
      "134\n",
      "996\n",
      "1\n",
      "2329\n",
      "1417\n",
      "499\n",
      "192\n",
      "1646\n",
      "1\n",
      "91\n",
      "1100\n",
      "528\n",
      "1415\n",
      "1108\n",
      "1066\n",
      "2797\n",
      "1\n",
      "2351\n",
      "2183\n",
      "258\n",
      "3438\n",
      "896\n",
      "1731\n",
      "991\n",
      "1177\n",
      "458\n",
      "2182\n",
      "1332\n",
      "2051\n",
      "3258\n",
      "795\n",
      "207\n",
      "1072\n",
      "726\n",
      "284\n",
      "4428\n",
      "717\n",
      "1351\n",
      "550\n",
      "3105\n",
      "1\n",
      "806\n",
      "426\n",
      "271\n",
      "1093\n",
      "940\n",
      "1968\n",
      "1343\n",
      "216\n",
      "1363\n",
      "255\n",
      "1\n",
      "1005\n",
      "3583\n",
      "2151\n",
      "103\n",
      "406\n",
      "1413\n",
      "5410\n",
      "638\n",
      "1548\n",
      "1610\n",
      "388\n",
      "1553\n",
      "2947\n",
      "1025\n",
      "518\n",
      "2489\n",
      "1737\n",
      "939\n",
      "1498\n",
      "2162\n",
      "1276\n",
      "213\n",
      "386\n",
      "444\n",
      "2050\n",
      "373\n",
      "1052\n",
      "2143\n",
      "367\n",
      "2300\n",
      "1501\n",
      "942\n",
      "2112\n",
      "871\n",
      "749\n",
      "368\n",
      "1\n",
      "71\n",
      "44\n",
      "1\n",
      "1126\n",
      "32\n",
      "3758\n",
      "1578\n",
      "219\n",
      "623\n",
      "1067\n",
      "294\n",
      "1342\n",
      "357\n",
      "594\n",
      "520\n",
      "640\n",
      "435\n",
      "1675\n",
      "1956\n",
      "2053\n",
      "1063\n",
      "794\n",
      "1544\n",
      "994\n",
      "1334\n",
      "1456\n",
      "4435\n",
      "2374\n",
      "1\n",
      "1\n",
      "925\n",
      "2546\n",
      "571\n",
      "1644\n",
      "532\n",
      "2639\n",
      "368\n",
      "1767\n",
      "105\n",
      "1562\n",
      "1440\n",
      "295\n",
      "2436\n",
      "888\n",
      "1654\n",
      "1560\n",
      "400\n",
      "3215\n",
      "193\n",
      "1596\n",
      "4289\n",
      "125\n",
      "936\n",
      "1\n",
      "3775\n",
      "243\n",
      "888\n",
      "3154\n",
      "1102\n",
      "2604\n",
      "2176\n",
      "821\n",
      "2022\n",
      "923\n",
      "591\n",
      "675\n",
      "1\n",
      "1727\n",
      "922\n",
      "1144\n",
      "1023\n",
      "1\n",
      "2046\n",
      "2365\n",
      "1716\n",
      "1063\n",
      "1\n",
      "2055\n",
      "1950\n",
      "535\n",
      "1470\n",
      "759\n",
      "145\n",
      "1240\n",
      "1301\n",
      "1049\n",
      "4791\n",
      "1732\n",
      "1140\n",
      "1216\n",
      "1198\n",
      "1817\n",
      "493\n",
      "1\n",
      "3434\n",
      "1081\n",
      "471\n",
      "2631\n",
      "2373\n",
      "1307\n",
      "2902\n",
      "482\n",
      "1041\n",
      "760\n",
      "1731\n",
      "64\n",
      "1063\n",
      "2390\n",
      "1132\n",
      "2325\n",
      "2309\n",
      "459\n",
      "1114\n",
      "772\n",
      "521\n",
      "310\n",
      "2203\n",
      "1028\n",
      "619\n",
      "327\n",
      "901\n",
      "1\n",
      "1\n",
      "1852\n",
      "1092\n",
      "210\n",
      "1103\n",
      "1203\n",
      "515\n",
      "1\n",
      "2578\n",
      "1\n",
      "881\n",
      "633\n",
      "1368\n",
      "1694\n",
      "2393\n",
      "519\n",
      "936\n",
      "172\n",
      "578\n",
      "1512\n",
      "3803\n",
      "319\n",
      "1978\n",
      "1645\n",
      "423\n",
      "359\n",
      "411\n",
      "2967\n",
      "2070\n",
      "1269\n",
      "339\n",
      "15\n",
      "775\n",
      "297\n",
      "304\n",
      "1830\n",
      "1750\n",
      "923\n",
      "3724\n",
      "304\n",
      "363\n",
      "2234\n",
      "2961\n",
      "325\n",
      "2150\n",
      "1146\n",
      "2324\n",
      "3034\n",
      "1574\n",
      "2357\n",
      "623\n",
      "1082\n",
      "324\n",
      "1365\n",
      "1407\n",
      "350\n",
      "1\n",
      "533\n",
      "1716\n",
      "2839\n",
      "1822\n",
      "3170\n",
      "141\n",
      "1385\n",
      "545\n",
      "1331\n",
      "451\n",
      "1067\n",
      "2014\n",
      "1363\n",
      "547\n",
      "366\n",
      "589\n",
      "168\n",
      "627\n",
      "2067\n",
      "1516\n",
      "1682\n",
      "955\n",
      "618\n",
      "923\n",
      "301\n",
      "1999\n",
      "351\n",
      "794\n",
      "1297\n",
      "1\n",
      "883\n",
      "1517\n",
      "235\n",
      "408\n",
      "1172\n",
      "1910\n",
      "329\n",
      "1370\n",
      "1098\n",
      "1508\n",
      "1234\n",
      "889\n",
      "636\n",
      "269\n",
      "1128\n",
      "1786\n",
      "655\n",
      "940\n",
      "1417\n",
      "2480\n",
      "197\n",
      "2532\n",
      "1612\n",
      "1245\n",
      "897\n",
      "895\n",
      "2677\n",
      "1215\n",
      "1835\n",
      "3396\n",
      "181\n",
      "552\n",
      "1527\n",
      "724\n",
      "1\n",
      "59\n",
      "679\n",
      "534\n",
      "293\n",
      "857\n",
      "535\n",
      "1318\n",
      "1034\n",
      "2006\n",
      "2270\n",
      "977\n",
      "508\n",
      "648\n",
      "226\n",
      "216\n",
      "1603\n",
      "565\n",
      "1\n",
      "2120\n",
      "1434\n",
      "1633\n",
      "703\n",
      "1931\n",
      "131\n",
      "1378\n",
      "828\n",
      "258\n",
      "2587\n",
      "131\n",
      "1247\n",
      "1\n",
      "1225\n",
      "788\n",
      "752\n",
      "1292\n",
      "214\n",
      "1709\n",
      "2743\n",
      "1669\n",
      "1882\n",
      "119\n",
      "763\n",
      "1316\n",
      "1372\n",
      "44\n",
      "1137\n",
      "1124\n",
      "659\n",
      "1\n",
      "3859\n",
      "1583\n",
      "1480\n",
      "693\n",
      "238\n",
      "2393\n",
      "1\n",
      "477\n",
      "1472\n",
      "1749\n",
      "1255\n",
      "3064\n",
      "1241\n",
      "1\n",
      "235\n",
      "1367\n",
      "547\n",
      "540\n",
      "2689\n",
      "1171\n",
      "56\n",
      "361\n",
      "1194\n",
      "194\n",
      "1308\n",
      "1062\n",
      "1195\n",
      "2633\n",
      "1150\n",
      "480\n",
      "869\n",
      "1\n",
      "143\n",
      "780\n",
      "698\n",
      "646\n",
      "1025\n",
      "2519\n",
      "277\n",
      "3628\n",
      "187\n",
      "1614\n",
      "999\n",
      "158\n",
      "1476\n",
      "797\n",
      "1292\n",
      "2214\n",
      "1258\n",
      "575\n",
      "425\n",
      "93\n",
      "269\n",
      "827\n",
      "56\n",
      "957\n",
      "900\n",
      "686\n",
      "256\n",
      "1793\n",
      "554\n",
      "464\n",
      "500\n",
      "1883\n",
      "269\n",
      "563\n",
      "1366\n",
      "1182\n",
      "1133\n",
      "438\n",
      "435\n",
      "484\n",
      "3515\n",
      "1558\n",
      "956\n",
      "1521\n",
      "1058\n",
      "1836\n",
      "72\n",
      "168\n",
      "1264\n",
      "3879\n",
      "58\n",
      "1034\n",
      "2027\n",
      "660\n",
      "1036\n",
      "1910\n",
      "731\n",
      "3456\n",
      "293\n",
      "1941\n",
      "795\n",
      "551\n",
      "1309\n",
      "477\n",
      "537\n",
      "1096\n",
      "1535\n",
      "1436\n",
      "745\n",
      "647\n",
      "1402\n",
      "1578\n",
      "1489\n",
      "606\n",
      "553\n",
      "1454\n",
      "2130\n",
      "2225\n",
      "2564\n",
      "1\n",
      "439\n",
      "469\n",
      "385\n",
      "1363\n",
      "1022\n",
      "1094\n",
      "0.0009838007184984688\n",
      "The optimal CV geometry, according to AI magic, 10000 tries: [ 72.15390766  82.99058432  21.2108769  718.9143833 ]\n",
      "The spectrum predicted by the model: [[0.03930048 0.94827491]]\n"
     ]
    }
   ],
   "source": [
    "#Check gradient descent for 10000 tries - better?\n",
    "g_min = X_train[0]\n",
    "loss_min = 1\n",
    "for x in X_train:\n",
    "    g = opt([0,1], nn_gs_2, x, delta, 0.01, mu_g, r, 1e-5, 1e4)\n",
    "    l = loss([0,1], nn_gs_2, g, mu_g, r)\n",
    "    if (l < loss_min):\n",
    "        g_min = g\n",
    "        loss_min = l\n",
    "print(loss_min)\n",
    "nn.predict([g_min])\n",
    "#Present optimal CV geometry\n",
    "g_min_ref = g_min\n",
    "print(\"The optimal CV geometry, according to AI magic, 10000 tries:\", scaler.inverse_transform(g_min))\n",
    "print(\"The spectrum predicted by the model:\", nn.predict([g_min]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8712ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
